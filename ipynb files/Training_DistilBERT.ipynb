{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4PEeZK7NB890"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qap9dIZ2C6fU"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def safe_label_parse(label_string):\n",
        "    return label_string.strip(\"[]\").replace(\"'\", \"\").split()\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/clean_labeled_zeroOne/train_label_dataset.csv')\n",
        "\n",
        "df['labels'] = df['labels'].apply(safe_label_parse)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eFdKraXTBauP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import ast\n",
        "import re\n",
        "\n",
        "def parse_list(s):\n",
        "    s = s.strip(\"[] \")\n",
        "    if ',' in s:\n",
        "        return list(map(int, s.split(',')))\n",
        "    else:\n",
        "        return list(map(int, s.split()))\n",
        "\n",
        "def parse_offset_mapping(s):\n",
        "\n",
        "    matches = re.findall(r'array\\(\\[(\\d+),\\s*(\\d+)\\]\\)', s)\n",
        "    return [[int(a), int(b)] for a, b in matches]\n",
        "\n",
        "\n",
        "df_tokenized = pd.read_csv('/content/drive/MyDrive/clean_labeled_zeroOne/train_tokenized_label_dataset_clean.csv')\n",
        "\n",
        "#Drop unnecessary columns\n",
        "columns_to_drop = ['full_date', 'min_fatalities', 'max_fatalities', 'countries']\n",
        "df_tokenized = df_tokenized.drop(columns=[col for col in columns_to_drop if col in df_tokenized.columns])\n",
        "\n",
        "#Clean up\n",
        "for col in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']:\n",
        "    if col in df_tokenized.columns:\n",
        "        df_tokenized[col] = df_tokenized[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
        "        df_tokenized[col] = df_tokenized[col].apply(parse_list)\n",
        "\n",
        "if 'offset_mapping' in df_tokenized.columns:\n",
        "    df_tokenized['offset_mapping'] = df_tokenized['offset_mapping'].astype(str).str.replace('\\n', ' ', regex=False)\n",
        "    df_tokenized['offset_mapping'] = df_tokenized['offset_mapping'].apply(parse_offset_mapping)\n",
        "\n",
        "df_tokenized = df_tokenized.reset_index(drop=True)\n",
        "\n",
        "train_tokenized = Dataset.from_pandas(df_tokenized)"
      ],
      "metadata": {
        "id": "ZzF8vnQaBbXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_tokenized[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Nac6h6YgBe-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pQ_dioVbBfkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Eval"
      ],
      "metadata": {
        "id": "vz3S9V_8BgE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "def safe_label_parse(label_string):\n",
        "    return label_string.strip(\"[]\").replace(\"'\", \"\").split()\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/clean_labeled_zeroOne/eval_label_dataset.csv')\n",
        "\n",
        "df['labels'] = df['labels'].apply(safe_label_parse)\n",
        "\n",
        "eval_dataset = Dataset.from_pandas(df)"
      ],
      "metadata": {
        "id": "TJKwbd7LBhHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset[0]"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1Vle1APDBjQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MFXmtpzGBkXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "import ast\n",
        "import re\n",
        "\n",
        "def parse_list(s):\n",
        "    s = s.strip(\"[] \")\n",
        "    if ',' in s:\n",
        "        return list(map(int, s.split(',')))\n",
        "    else:\n",
        "        return list(map(int, s.split()))\n",
        "\n",
        "def parse_offset_mapping(s):\n",
        "\n",
        "    matches = re.findall(r'array\\(\\[(\\d+),\\s*(\\d+)\\]\\)', s)\n",
        "    return [[int(a), int(b)] for a, b in matches]\n",
        "\n",
        "\n",
        "df_tokenized = pd.read_csv('/content/drive/MyDrive/clean_labeled_zeroOne/eval_tokenized_label_dataset_clean.csv')\n",
        "\n",
        "#Drop unnecessary columns\n",
        "columns_to_drop = ['full_date', 'min_fatalities', 'max_fatalities', 'countries']\n",
        "df_tokenized = df_tokenized.drop(columns=[col for col in columns_to_drop if col in df_tokenized.columns])\n",
        "\n",
        "#Clean up\n",
        "for col in ['input_ids', 'attention_mask', 'token_type_ids', 'labels']:\n",
        "    if col in df_tokenized.columns:\n",
        "        df_tokenized[col] = df_tokenized[col].astype(str).str.replace('\\n', ' ', regex=False)\n",
        "        df_tokenized[col] = df_tokenized[col].apply(parse_list)\n",
        "\n",
        "if 'offset_mapping' in df_tokenized.columns:\n",
        "    df_tokenized['offset_mapping'] = df_tokenized['offset_mapping'].astype(str).str.replace('\\n', ' ', regex=False)\n",
        "    df_tokenized['offset_mapping'] = df_tokenized['offset_mapping'].apply(parse_offset_mapping)\n",
        "\n",
        "df_tokenized = df_tokenized.reset_index(drop=True)\n",
        "\n",
        "eval_tokenized = Dataset.from_pandas(df_tokenized)"
      ],
      "metadata": {
        "id": "RyBONvt7Bkqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHyz7Qm7BoHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "0U_jnmrfBobn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7JuKCwamCCtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6PZp7-VqDDGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from seqeval.metrics import classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from datetime import datetime\n",
        "import spacy\n",
        "id2label\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "label_list = [\n",
        "    \"O\",         # Outside of any entity\n",
        "    \"B-DATE\",    # Beginning of date\n",
        "    \"I-DATE\",    # Inside of date\n",
        "    \"B-MIN_FAT\", # Beginning of min fatalities\n",
        "    \"I-MIN_FAT\", # Inside of min fatalities\n",
        "    \"B-MAX_FAT\", # Beginning of max fatalities\n",
        "    \"I-MAX_FAT\", # Inside of max fatalities\n",
        "    \"B-COUNTRY\", # Beginning of country\n",
        "    \"I-COUNTRY\"  # Inside of country\n",
        "]\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "# Tokenizer- DistilBERT for efficiency\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "#tokenize and align labels\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=512, return_offsets_mapping=True)\n",
        "    labels = []\n",
        "\n",
        "    for i, label in enumerate(examples[\"labels\"]):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label[word_idx]])\n",
        "            else:\n",
        "                label_ids.append(label2id[label[word_idx]] if label[word_idx].startswith(\"I\") else -100)\n",
        "            previous_word_idx = word_idx\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269,
          "referenced_widgets": [
            "5b6c4c97a14b445193517e9342bbd8d4",
            "49bb059b373941a38318ccaed2739fef",
            "72b66fe6c0214e5cbf44eb7eaa2e9eab",
            "15f5eebf541b44be93dfc91775062b07",
            "bde1bb8d65804413a767ebcb02842472",
            "372f9ca2f1604badba27b1f8ef7c3bce",
            "673ea446cee74234987564c48470813c",
            "893a5ff4cfad4803b783d16d3331855e",
            "4a9d70f399284ba396838eabcff2c69e",
            "0b3c03bc211742a1af0b762f57b60d41",
            "1a52c5ead94d45faa523800a8b1403fb",
            "00a08e32563a47279334fe64aac3e6df",
            "94605c80988d4304b71a3d7d8307a7e6",
            "a328429bf2c24ee99bbaedb67f592222",
            "65aab1dde5db4a62bbe6bf026e774cfc",
            "fef7b6a883674ea4a037fee564282710",
            "02cf9ca5daec40dbaca6ff635b62315f",
            "828ffa2cc0f14cc988590358f44bc393",
            "f368b5596437483b8f319ea9a38bb8bf",
            "24067e3884f442c48302acb8cde2af49",
            "e8b92a5da37d4d3aac1bd3e66fba8bf2",
            "4096aee2f3f84f0eba60126db7a6f474",
            "781849712dc648798f4877def62f622b",
            "c509fb6a81204ebc8053e103333310af",
            "eaaf78b7e21b4ef4b73aa87ab5407d27",
            "43769a8f27b14e7c83c47994c12bc440",
            "d592a59b01394dc0b7022bffdbc827fe",
            "912955492cdb4c15bb18ebe7aed1a61d",
            "990bc1acc8b74b9b8d328feae8ceaf46",
            "0d62ad3522344460a4d7ea1ed35130c9",
            "1342b087f14043ef9a88214218446e26",
            "3aeb300e688340edb8870240e74ab17b",
            "30e13b4ebe4a492495c3630b1b59833c",
            "b43105d1b428491d851fd8072c43f0b6",
            "7680f00ef1f9477caabfaa33e5dbb8b7",
            "9fcc69f7a8ac40f8bd9ce71c43001236",
            "5b60322511d547c7b6e8ca5cd987e071",
            "9e55ba63fd3e4b789ab92ec2c121e2dd",
            "247efb3ef94642c5a9632bcf962fea48",
            "4fac0fde437540c082d19c780dbed565",
            "aedf0f3af7a34f4dae4c5ae0120e291e",
            "1c2db277ea2d4dafbfc584f14b20923c",
            "c1ef91985f1c431abbcef5bbbc45c67d",
            "a806298f2bbd4972bab29843e262ccfe"
          ]
        },
        "collapsed": true,
        "id": "o84SQDchDBTa",
        "outputId": "34a641fc-cfe7-40cb-ce7a-94974c726c8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b6c4c97a14b445193517e9342bbd8d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a08e32563a47279334fe64aac3e6df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "781849712dc648798f4877def62f622b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b43105d1b428491d851fd8072c43f0b6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id2label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxbS0VetDnIl",
        "outputId": "b80b708c-0b77-40b4-f0c7-552229ca0486"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'O',\n",
              " 1: 'B-DATE',\n",
              " 2: 'I-DATE',\n",
              " 3: 'B-MIN_FAT',\n",
              " 4: 'I-MIN_FAT',\n",
              " 5: 'B-MAX_FAT',\n",
              " 6: 'I-MAX_FAT',\n",
              " 7: 'B-COUNTRY',\n",
              " 8: 'I-COUNTRY'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Initialize model\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_name,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "# Trainer with both train and eval datasets\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_tokenized,\n",
        "    eval_dataset=eval_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "dAff3xW0Bpmm",
        "outputId": "a6c73ed4-97a9-4856-92f0-d118210d64c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-21-fe07f4c00c17>:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='8088' max='8088' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [8088/8088 1:50:57, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.021500</td>\n",
              "      <td>0.016866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.013300</td>\n",
              "      <td>0.012267</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.013700</td>\n",
              "      <td>0.011554</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=8088, training_loss=0.018810446579543585, metrics={'train_runtime': 6658.3187, 'train_samples_per_second': 19.435, 'train_steps_per_second': 1.215, 'total_flos': 1.6908912316348416e+16, 'train_loss': 0.018810446579543585, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/test_df_for_new.csv\")"
      ],
      "metadata": {
        "id": "CPmFIqV0iYrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install word2number"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U1-GWQghi6h7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from dateutil import parser\n",
        "from word2number import w2n\n",
        "\n",
        "def convert_number_to_words(number):\n",
        "    try:\n",
        "        return w2n.word_to_num(str(number))\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def find_all_spans(text, target):\n",
        "    matches = [m for m in re.finditer(re.escape(target.lower()), text.lower())]\n",
        "    return [(m.start(), m.end()) for m in matches]\n",
        "\n",
        "def prepare_labels(text, date, min_fat, max_fat, countries):\n",
        "    doc = nlp(text)\n",
        "    labels = [\"O\"] * len(doc)\n",
        "\n",
        "    def match_and_tag(value, tag):\n",
        "        for span_start, span_end in find_all_spans(text, value):\n",
        "            span = doc.char_span(span_start, span_end)\n",
        "            if span:\n",
        "                labels[span.start] = f\"B-{tag}\"\n",
        "                for i in range(span.start + 1, span.end):\n",
        "                    labels[i] = f\"I-{tag}\"\n",
        "\n",
        "    # --- DATE tagging ---\n",
        "    try:\n",
        "        parsed = datetime.strptime(date, \"%Y-%m-%d\")\n",
        "        date_variants = [\n",
        "            parsed.strftime(\"%B %d, %Y\"),\n",
        "            parsed.strftime(\"%b %d, %Y\"),\n",
        "            parsed.strftime(\"%d %B %Y\"),\n",
        "            parsed.strftime(\"%d %b %Y\"),\n",
        "            parsed.strftime(\"%B %d %Y\"),\n",
        "            parsed.strftime(\"%b %d %Y\"),\n",
        "            parsed.strftime(\"%Y-%m-%d\"),\n",
        "            parsed.strftime(\"%Y%m%d\"),\n",
        "            parsed.strftime(\"%d%B%Y\"),\n",
        "            parsed.strftime(\"%d%b%Y\"),\n",
        "        ]\n",
        "        for date_str in date_variants:\n",
        "            match_and_tag(date_str, \"DATE\")\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "    for number, tag in [(min_fat, \"MIN_FAT\"), (max_fat, \"MAX_FAT\")]:\n",
        "        if pd.isna(number):\n",
        "            continue\n",
        "        str_num = str(int(number))\n",
        "        word_num = None\n",
        "        try:\n",
        "            word_num = w2n.word_to_num(str_num)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "\n",
        "        match_and_tag(str_num, tag)\n",
        "        try:\n",
        "            word = list(w2n.american_number_system.keys())[int(str_num)]\n",
        "            match_and_tag(word, tag)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    if countries and isinstance(countries, str):\n",
        "        for country in countries.split(\",\"):\n",
        "            country = country.strip()\n",
        "            if country:\n",
        "                match_and_tag(country, \"COUNTRY\")\n",
        "\n",
        "    return labels"
      ],
      "metadata": {
        "id": "qp5x7I92imtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NkfEK3OsjIFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scores"
      ],
      "metadata": {
        "id": "clNWhsGUDnDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report, f1_score\n",
        "\n",
        "def evaluate_model(test_df, model, tokenizer):\n",
        "    \"\"\"Comprehensive evaluation of the trained model\"\"\"\n",
        "\n",
        "    test_data = []\n",
        "    for _, row in test_df.iterrows():\n",
        "        labels = prepare_labels(\n",
        "            row['cleaned_text'],\n",
        "            row['full_date'],\n",
        "            row['min_fatalities'],\n",
        "            row['max_fatalities'],\n",
        "            row['countries']\n",
        "        )\n",
        "        test_data.append({\n",
        "            \"text\": row['cleaned_text'],\n",
        "            \"labels\": labels\n",
        "        })\n",
        "\n",
        "    test_dataset = Dataset.from_pandas(pd.DataFrame(test_data))\n",
        "    test_tokenized = test_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "    predictions = trainer.predict(test_tokenized)\n",
        "    preds = np.argmax(predictions.predictions, axis=2)\n",
        "\n",
        "    true_labels = [[id2label[l] for l in label if l != -100] for label in test_tokenized[\"labels\"]]\n",
        "    pred_labels = [[id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "                  for prediction, label in zip(preds, test_tokenized[\"labels\"])]\n",
        "\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(true_labels, pred_labels))\n",
        "\n",
        "\n",
        "    overall_f1 = f1_score(true_labels, pred_labels)\n",
        "    print(f\"\\nOverall F1 Score: {overall_f1:.4f}\")\n",
        "\n",
        "    return true_labels, pred_labels\n",
        "\n",
        "\n",
        "true_labels, pred_labels = evaluate_model(test_df, model, tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "5cfb9611e4654faab12f4aa48f88bfb9",
            "b1b9e7904e4c4feab8184cb788c0302b",
            "5aa8c91207694050989cf78399cc561d",
            "025604e152f841008cde7f51f4ae2c8b",
            "60d94faf26914afc8c0b3d7d3a3f1045",
            "5d3ec82e0a65486ab04334c1dc6c1fea",
            "cabe3660bbca4798a4a57ca2d6906b8c",
            "06a21110fb924a6d948cd2b6b127aa70",
            "698fef85c76f47a996699782ba6d94f7",
            "263355b5678048ceb307a628f99ff68d",
            "2f235e5b14f14123b26a7e988f65ac9d"
          ]
        },
        "id": "aVYZz9Bxdl51",
        "outputId": "79205b6f-7524-4793-cb05-5c84390b1ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10784 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5cfb9611e4654faab12f4aa48f88bfb9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     COUNTRY       0.95      0.97      0.96     51159\n",
            "        DATE       1.00      1.00      1.00     18587\n",
            "     MAX_FAT       0.84      0.85      0.84      7401\n",
            "     MIN_FAT       0.78      0.83      0.81      4740\n",
            "\n",
            "   micro avg       0.94      0.96      0.95     81887\n",
            "   macro avg       0.89      0.91      0.90     81887\n",
            "weighted avg       0.94      0.96      0.95     81887\n",
            "\n",
            "\n",
            "Overall F1 Score: 0.9498\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./conflict_info_extractor\")\n",
        "tokenizer.save_pretrained(\"./conflict_info_extractor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAiS8t0NlrSC",
        "outputId": "141f1d7c-40ee-4ba3-f8f5-fc637a87606e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./conflict_info_extractor/tokenizer_config.json',\n",
              " './conflict_info_extractor/special_tokens_map.json',\n",
              " './conflict_info_extractor/vocab.txt',\n",
              " './conflict_info_extractor/added_tokens.json',\n",
              " './conflict_info_extractor/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NUkHGWzVDilr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Manual Testing"
      ],
      "metadata": {
        "id": "fv6YXK0UDiRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Basic text cleaning function\"\"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "e2OIyD--nIOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import pipeline\n",
        "\n",
        "class ConflictInfoExtractor:\n",
        "    def __init__(self, model_path=\"./conflict_info_extractor\"):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        self.classifier = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "\n",
        "    def extract_info(self, text):\n",
        "        \"\"\"Clean and process text for extraction\"\"\"\n",
        "\n",
        "        text = clean_text(text)\n",
        "\n",
        "        # Get model predictions\n",
        "        entities = self.classifier(text)\n",
        "\n",
        "        # Process entities into structured format\n",
        "        result = {\n",
        "            \"date\": \"\",\n",
        "            \"min_fatalities\": \"\",\n",
        "            \"max_fatalities\": \"\",\n",
        "            \"countries\": []\n",
        "        }\n",
        "\n",
        "        for entity in entities:\n",
        "            if entity[\"entity_group\"] == \"DATE\":\n",
        "                result[\"date\"] = self._format_date(entity[\"word\"])\n",
        "            elif entity[\"entity_group\"] == \"MIN_FAT\":\n",
        "                result[\"min_fatalities\"] = entity[\"word\"]\n",
        "            elif entity[\"entity_group\"] == \"MAX_FAT\":\n",
        "                result[\"max_fatalities\"] = entity[\"word\"]\n",
        "            elif entity[\"entity_group\"] == \"COUNTRY\":\n",
        "                result[\"countries\"].append(entity[\"word\"])\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _format_date(self, date_str):\n",
        "        \"\"\"Standardize date formats\"\"\"\n",
        "        try:\n",
        "            for fmt in [\"%B %d, %Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%d-%m-%Y\"]:\n",
        "                try:\n",
        "                    dt = datetime.strptime(date_str, fmt)\n",
        "                    return dt.strftime(\"%Y-%m-%d\")\n",
        "                except ValueError:\n",
        "                    continue\n",
        "            return date_str\n",
        "        except:\n",
        "            return date_str\n",
        "\n",
        "\n",
        "extractor = ConflictInfoExtractor()\n",
        "\n",
        "sample_text = \"20051005 0013-05october 2005 ln np1.txt-files.list suicide blast hits canadian convoy afghanistan , child killed afg-can october 5 , 2005 wednesday 1123 gmt news source c agence france presse english svm score 2.764 suicide bomber pick-up truck blew near canadian military convoy volatile southern afghanistan wednesday , killing afghan child , provincial governor said . second suicide attack week insurgency-hit afghanistan , raising fears rebels influenced iraq-style tactics foreign targets . canadian force said three soldiers lightly wounded attack three-jeep convoy outskirts southern city kandahar , birthplace ousted taliban regime . kandahar province governor asadullah khalid told reporters attack targeted canadians . suicide attack one afghan boy killed , man wounded course suicide bomber also killed , told reporters . canadians part military civilian reconstruction team based kandahar since early august , teams publicity officer captain francois giroux told afp . morning convoy hit vehicle-borne explosive improvised device detonated suicide bomber . three canadian soldiers superficial injuries , mostly minor burns muscle-ache , said . told civilian fatalities details , said . giroux said one jeeps damaged attack , first reconstruction team since deployment kandahar . knew came environment hostile soldiers well-trained know job , giroux said . although suicide bombings rare afghanistan , last week ago man motorcycle blew crowd soldiers knocked duty base kabul . eight soldiers civilian killed . kabul september , two canadian soldiers suffered minor injuries blast hit jeep patrol . kandahar powerbase taliban regime toppled late 2001 waging insurgency us-backed government president hamid karzai . province one several southern eastern afghanistan seen worst increasingly deadly insurgency 1,300 people killed year , 850 last year . 250 canadians kandahar reconstruction team , giroux said . soldiers , also surgeons police officers present . teams , officially known provincial reconstruction teams , set across afghanistan build ties local officials help reconstruction war-torn region . canadian soldiers also part 10,500-strong nato force helping maintain security afghanistans capital kabul northern western parts country . str-brdkth\"\n",
        "print(extractor.extract_info(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnioVFnGmCSi",
        "outputId": "5daf918b-6378-4556-994a-36c6942a868c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'date': '##100', 'min_fatalities': 'one', 'max_fatalities': '1300', 'countries': ['afghanistan', 'afghanistan', 'afghanistan', 'afghanistan', 'afghanistan', 'afghanistan']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"afg-irn afg-irn afg-irn afg-irn afg-irn afg-irn afg-irn afg-irn 20050505 0021-may05 2005 ln np1.txt-files.list taleban , afghan government casualty figures differ clash south afg-irn may 5 , 2005 , thursday news source c bbc monitoring south asia - political svm score 3.967 supplied bbc worldwide monitoring source voice islamic republic iran , mashhad , dari 1330 gmt , 5 may 05 excerpt report iranian radio mashhad 5 may spokesman international peacekeeping forces afghanistan says 44 people killed clash kandahar province last night . spokesman told reporter people killed affiliated taleban . coalition forces afghanistan said yesterday 20 insurgents one private killed military operation daichupan area zabol province previous day . added six us soldiers five afghan privates sustained injuries . meantime , spokesman taleban , mofti latifollah hakimi , said five taleban fighters killed clash , addition 16 civilians , including number women children . hakimi claimed eight foreign soldiers 15 afghan privates killed operation . spokesman governor zabol province said found bodies three foreigners , including two chechens one pakistani . correspondent kandahar also reported gunmen killed nine soldiers national army injured three others . passage omitted repetition details correspondent kandahar\"\n",
        "print(extractor.extract_info(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkbNoKiPmv2x",
        "outputId": "01d6055f-64c7-42e9-a73d-ecd5589f8e80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'date': '##50', 'min_fatalities': '5', 'max_fatalities': '44', 'countries': ['iran', 'afghanistan', 'afghanistan']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"20090214 0017-feb14 2009 ln np1.txt-files.list australia 5 children killed afghan battle afg-aul february 14 , 2009 saturday 138 gmt news source c associated press worldstream svm score 2.836 gunfight australian forces taliban fighters southern afghanistan killed five children caught crossfire , australian defense ministry said . afghan officials gave lower death tolls . asadullah hamdan , provincial governor , said friday three children 7 10 years old killed . fighting southern uruzgan province started raid international afghan troops compounds village insurgent leaders believed holed , nato said statement . australian defense ministry said reports five children killed four people wounded two children . provincial police chief gen. juma gul himat said reports four children killed . conflicting death tolls could resolved . one insurgent also killed , australian ministry statement said . australian troops wounded . total 1,162 civilians killed insurgency-related incidents 2008 , according ap casualty count 368 foreign afghan troops 768 taliban . another 26 caught crossfire . deaths thursday came newly appointed u.s. envoy region toured afghanistan . envoy richard holbrooke first visit country since appointed president barack obama define new strategy combat taliban afghanistan pakistan . friday , discussed counterterrorism strategy countrys two vice presidents pledged continued u.s. support , state television reported . holbrooke also expected meet president hamid karzai . karzai repeatedly warned western forces need prevent civilian deaths lose support afghan people .\"\n",
        "print(extractor.extract_info(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_Jl07ZkotCl",
        "outputId": "61efeab4-f031-4a86-84f8-b2f29200db30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'date': 'february 14 2009', 'min_fatalities': 'one', 'max_fatalities': '##2', 'countries': ['australia', 'afghanistan', 'afghanistan', 'afghanistan', 'pakistan']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"key cf27d73d 4b0e 46ea 85ca e2d8b96c6c60 collection users vjdorazio desktop mid mid5 mid 5 0 lexis nexis reports mid5_2014 bbc_2014 march 2014 17 20march_2014_ln_np1 txt headline karabakh denies involvement death azeri soldier contact line date 20140317 source bbc monitoring trans caucasus unit dateline dateline found googleddateline dateline found americandateline false byline byline found language english subject subject found organization organization found geographic geographic found loaddate march 18 2014 pubtype transcript countries united kingdom 1 azerbaijan 2 armenia 4 supplied bbc worldwide monitoring text report armenian internet news agency news march 17 nagornyy karabakh defence army denied involvement death azerbaijani soldier reported azeri media nagornyy karabakh defence army remains committed cease fire regime frontline units violate truce reports azerbaijan involvement nagornyy karabakh republic armed forces death azeri soldier disinformation nagornyy karabakh defence army press secretary senor hasratyan said azerbaijani media claim soldier elvin hasanov aged 20 killed near fizuli soldier posthumously awarded third degree medal valor outstanding military service exhibited performance combat bbcm note armenia azerbaijan locked conflict armenian populated region nagornyy karabakh came armenian control war ended ceasefire 1994 peace agreement countries signed cease fire violations common occasionally bringing casualties sides accuse source news yerevan armenian 1257gmt 17 mar 14 copyright 2014 british broadcasting corporation rights reserved\"\n",
        "print(extractor.extract_info(sample_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe6egWEVpH3r",
        "outputId": "5088e26f-1149-44da-958f-31adb113aacb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'date': 'march 18 2014', 'min_fatalities': '', 'max_fatalities': '20', 'countries': ['azerbaijan', 'armenia', 'united kingdom']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Using CSV"
      ],
      "metadata": {
        "id": "dIfTuDWjp8xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "from datetime import datetime\n",
        "\n",
        "class ConflictInfoExtractor:\n",
        "    def __init__(self, model_path=\"./conflict_info_extractor\"):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        self.classifier = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        return text.replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    def extract_info(self, text):\n",
        "        text = self.clean_text(text)\n",
        "        entities = self.classifier(text)\n",
        "\n",
        "        result = {\n",
        "            \"date\": \"\",\n",
        "            \"min_fatalities\": \"\",\n",
        "            \"max_fatalities\": \"\",\n",
        "            \"countries\": []\n",
        "        }\n",
        "\n",
        "        for entity in entities:\n",
        "            word = entity[\"word\"].replace(\"#\", \"\")\n",
        "            if entity[\"entity_group\"] == \"DATE\":\n",
        "                result[\"date\"] = self._format_date(word)\n",
        "            elif entity[\"entity_group\"] == \"MIN_FAT\":\n",
        "                result[\"min_fatalities\"] = word\n",
        "            elif entity[\"entity_group\"] == \"MAX_FAT\":\n",
        "                result[\"max_fatalities\"] = word\n",
        "            elif entity[\"entity_group\"] == \"COUNTRY\":\n",
        "                result[\"countries\"].append(word.lower())\n",
        "\n",
        "        result[\"countries\"] = list(set(result[\"countries\"]))\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _format_date(self, date_str):\n",
        "        for fmt in [\"%B %d, %Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%d-%m-%Y\"]:\n",
        "            try:\n",
        "                dt = datetime.strptime(date_str, fmt)\n",
        "                return dt.strftime(\"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "        return date_str\n",
        "\n",
        "    def process_csv(self, csv_path, text_column=\"text\"):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df[text_column] = df[text_column].fillna(\"\").astype(str)\n",
        "\n",
        "        results = []\n",
        "        for text in df[text_column]:\n",
        "            info = self.extract_info(text)\n",
        "            results.append(info)\n",
        "\n",
        "        return pd.DataFrame(results)"
      ],
      "metadata": {
        "id": "vrLF7LQPp-3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = ConflictInfoExtractor()\n",
        "df_results = extractor.process_csv(\"/content/drive/MyDrive/2005.csv\", text_column=\"text\")\n",
        "print(df_results.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAQDBPAUqAqN",
        "outputId": "73e39e95-38a0-4ca3-890b-a5fd97174be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   date min_fatalities max_fatalities  \\\n",
            "0  2005                                 \n",
            "1    70                                 \n",
            "2  2005                                 \n",
            "3    70                          dead   \n",
            "4  2005            two            600   \n",
            "\n",
            "                                           countries  \n",
            "0  [play, solomon islands, australia, hill, recen...  \n",
            "1  [would, australia, afghanistan, ., quite, ,, s...  \n",
            "2  [australia, afghanistan, quite, howard, taken,...  \n",
            "3  [s, australia, howard, 900, recent, law, troop...  \n",
            "4                     [australia, afghanistan, iraq]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuYbokMKrdUe",
        "outputId": "222f2897-e3bf-4235-deba-96775b7f02a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8180, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_results.to_csv(\"/content/drive/MyDrive/test_results\", index=False)"
      ],
      "metadata": {
        "id": "yel_Ilh2tP6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#More Cleaning"
      ],
      "metadata": {
        "id": "TNGRUXaEvLmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/content/drive/MyDrive/states2016.csv\")"
      ],
      "metadata": {
        "id": "i7MwjGnGy9Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "statename_column = df2['statenme']\n",
        "statename_column"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "mPd-YeStzC0X",
        "outputId": "dbdc2b01-637e-472c-fa42-ceded54d485e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            United States of America\n",
              "1                              Canada\n",
              "2                             Bahamas\n",
              "3                                Cuba\n",
              "4                                Cuba\n",
              "                    ...              \n",
              "238                             Nauru\n",
              "239                  Marshall Islands\n",
              "240                             Palau\n",
              "241    Federated States of Micronesia\n",
              "242                             Samoa\n",
              "Name: statenme, Length: 243, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>statenme</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>United States of America</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Bahamas</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Cuba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Cuba</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>238</th>\n",
              "      <td>Nauru</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>Marshall Islands</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>240</th>\n",
              "      <td>Palau</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241</th>\n",
              "      <td>Federated States of Micronesia</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>242</th>\n",
              "      <td>Samoa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>243 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from word2number import w2n\n",
        "\n",
        "class ConflictInfoExtractor:\n",
        "    def __init__(self, model_path=\"./conflict_info_extractor\", valid_states=None):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        self.model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "        self.classifier = pipeline(\n",
        "            \"token-classification\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "            aggregation_strategy=\"simple\"\n",
        "        )\n",
        "        self.valid_states = set(state.lower() for state in valid_states) if valid_states else set()\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        return str(text).replace(\"\\n\", \" \").strip()\n",
        "\n",
        "    def _format_date(self, date_str):\n",
        "        for fmt in [\"%B %d, %Y\", \"%Y-%m-%d\", \"%m/%d/%Y\", \"%d-%m-%Y\"]:\n",
        "            try:\n",
        "                dt = datetime.strptime(date_str, fmt)\n",
        "                return dt.strftime(\"%Y-%m-%d\")\n",
        "            except ValueError:\n",
        "                continue\n",
        "        return date_str\n",
        "\n",
        "    def _convert_to_number(self, word):\n",
        "        try:\n",
        "\n",
        "            return str(w2n.word_to_num(word))\n",
        "        except:\n",
        "\n",
        "            if word.isdigit():\n",
        "                return word\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "    def process_csv(self, csv_path, text_column=\"text\"):\n",
        "        df = pd.read_csv(csv_path)\n",
        "        df[text_column] = df[text_column].fillna(\"\").astype(str)\n",
        "\n",
        "        results = []\n",
        "        for text in df[text_column]:\n",
        "            info = self.extract_info(text)\n",
        "            results.append(info)\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "    def extract_info(self, text):\n",
        "        text = self.clean_text(text)\n",
        "        entities = self.classifier(text)\n",
        "\n",
        "        result = {\n",
        "            \"date\": \"\",\n",
        "            \"min_fatalities\": \"\",\n",
        "            \"max_fatalities\": \"\",\n",
        "            \"countries\": []\n",
        "        }\n",
        "        date_parts = []\n",
        "\n",
        "        for entity in entities:\n",
        "            word = entity[\"word\"].replace(\"#\", \"\").strip(\" ,.-\").lower()\n",
        "            if not word or word in [',', '-', '.', '']:\n",
        "                continue\n",
        "\n",
        "            if entity[\"entity_group\"] == \"DATE\":\n",
        "                date_parts.append(word)\n",
        "            elif entity[\"entity_group\"] == \"MIN_FAT\":\n",
        "                result[\"min_fatalities\"] = self._convert_to_number(word)\n",
        "            elif entity[\"entity_group\"] == \"MAX_FAT\":\n",
        "                result[\"max_fatalities\"] = self._convert_to_number(word)\n",
        "            elif entity[\"entity_group\"] == \"COUNTRY\" and word in self.valid_states:\n",
        "                result[\"countries\"].append(word)\n",
        "\n",
        "\n",
        "        full_date = \" \".join(date_parts).strip()\n",
        "        result[\"date\"] = self._format_date(full_date)\n",
        "\n",
        "\n",
        "        result[\"countries\"] = sorted(set(result[\"countries\"]))\n",
        "        result[\"min_fatalities\"] = result[\"min_fatalities\"] if result[\"min_fatalities\"] else \"0\"\n",
        "        result[\"max_fatalities\"] = result[\"max_fatalities\"] if result[\"max_fatalities\"] else \"0\"\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "nyqk3tUavItP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_states = df2['statenme'].dropna().str.lower().tolist()\n",
        "\n",
        "extractor = ConflictInfoExtractor(valid_states=valid_states)\n",
        "\n",
        "df_results = extractor.process_csv(\"/content/drive/MyDrive/2005.csv\", text_column=\"text\")\n",
        "\n",
        "#cleaned version\n",
        "df_results[\"countries\"] = df_results[\"countries\"].apply(lambda x: \"; \".join(x))\n",
        "df_results.to_csv(\"cleaned_conflict_info3.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cRBYhOE8vN-Q",
        "outputId": "ddfa131e-e262-4647-db2e-0e0a9b16af4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "US5S-6Uavi77"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}